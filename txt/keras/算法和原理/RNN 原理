
（1）RNN的基础：
St=f(U∗Xt+W∗St−1)
Xt:表示t时刻的输入，ot:表示t时刻的输出，St:表示t时刻的记忆,f()是神经网络中的激活函数
运用softmax来预测每个输出出现的概率：
ot=softmax(VSt)

RNN中的结构细节：
1.可以把St当作隐状态，捕捉了之前时间点上的信息。就像你去考研一样，考的时候记住了你能记住的所有信息。
2.ot是由当前时间以及之前所有的记忆得到的。就是你考研之后做的考试卷子，是用你的记忆得到的。
3.很可惜的是，St并不能捕捉之前所有时间点的信息。就像你考研不能记住所有的英语单词一样。
4.和卷积神经网络一样，这里的网络中每个cell都共享了一组参数（U，V，W）,这样就能极大的降低计算量了。
5.ot在很多情况下都是不存在的，因为很多任务，比如文本情感分析，都是只关注最后的结果的。就像考研之后选择学校，学校不会管你到底怎么努力，怎么心酸的准备考研，而只关注你最后考了多少分。



（2）RNN的改进1：双向RNN
从前往后：S1t→=f(U1→∗Xt+W1→∗St−1+b1→)

从后往前:S2t→=f(U2∗Xt→+W2→∗St−1+b2→)
输出：ot=softmax(V∗[S1t→;S2t→])
这里的[S1t→;S2t→]做的是一个拼接，如果他们都是1000X1维的，拼接在一起就是1000X2维的了。
双向RNN需要的内存是单向RNN的两倍，因为在同一时间点，双向RNN需要保存两个方向上的权重参数，在分类的时候，需要同时输入两个隐藏层输出的信息。



（3）RNN的改进2：深层双向RNN
深层双向RNN 与双向RNN相比，多了几个隐藏层





RNN的缺陷和LSTM的应用
如果把RNN展开，可以看到RNN的结构和普通的网络相似，可以将其看作是若干个相同的网络相连，并将信息在网络中进行传递。由于这种信息传递的存在，RNN的就可以根据之前出现的信息对当前的信息进行推断，特别是在语言处理中，RNN就可以用于根据上文预测下一个将要出现的词。但是RNN只能处理一定间隔的信息，如果上文间隔过远，就有可能出现难以联想的情况。比如：
I grew up in France...此处省略100个字....I speak fluent French.这种提示信息France距离French就间隔过远，RNN处理起来就比较吃力了。理论上RNN是能够通过一些人为的参数设置来实现这种大间隔的处理的，但是RNN似乎不能够去通过学习实现。《Understanding LSTM Networks》还列举了两篇论文来证明这一点。
因此，1997年有人就提出了LSTM（Long Short Term Memory Network），这是一种特殊的RNN，按照文章的说法，LSTM生来就是能够长时间记忆的网络，不需要刻意学习。

在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。Bengio, et al.
(1994)等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。



https://blog.csdn.net/qq_39422642/article/details/78676567  ``