所有的Keras层对象都有如下方法：
layer.get_weights()：返回层的权重（numpy array）

layer.set_weights(weights)：从numpy array中将权重加载到该层中，要求numpy array的形状与* layer.get_weights()的形状相同

layer.get_config()：返回当前层配置信息的字典，层也可以借由配置信息重构




如果层仅有一个计算节点（即该层不是共享层），则可以通过下列方法获得输入张量、输出张量、输入数据的形状和输出数据的形状：
layer.input
layer.output
layer.input_shape
layer.output_shape


如果该层有多个计算节点（参考层计算节点和共享层）。可以使用下面的方法
layer.get_input_at(node_index)
layer.get_output_at(node_index)
layer.get_input_shape_at(node_index)
layer.get_output_shape_at(node_index)




Core

全连接层：Dense

Activation层：对一个层的输出添加激活函数

Dropout层：每次更新参数的时候随机断开一定百分比(b)的输入神经元连接，用于防止过拟合

Flatten层：用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。

Reshape层：用来将输入shape转换为特定的shape

Permute层：将输入的维度按照给定模式进行重排，例如，当需要将RNN和CNN网络连接时，可能会用到该层。

RepeatVector层：RepeatVector层将输入重复n次

Merge层：Merge层根据给定的模式，将一个张量列表中的若干张量合并为一个单独的张量

Lambda层：本函数用以对上一层的输出施以任何Theano/TensorFlow表达式

ActivityRegularizer层：经过本层的数据不会有任何变化，但会基于其激活值更新损失函数值

Masking层：使用给定的值对输入的序列信号进行“屏蔽”，用以定位需要跳过的时间步。

实例参考keras文档，有详细的说明
Highway层：Highway层建立全连接的Highway网络，这是LSTM在前馈神经网络中的推广

MaxoutDense层：参数尚不理解，具体参考文献和文档。

Convolution

Convolution2D层：二维卷积层对二维输入进行滑动窗卷积

AtrousConvolution2D层：该层对二维输入进行Atrous卷积，也即膨胀卷积或带孔洞的卷积。

Convolution1D, AtrousConvolution1D，Convolution3D同
SeparableConvolution2D层：该层是对2D输入的可分离卷积。可分离卷积首先按深度方向进行卷积（对每个输入通道分别卷积），然后逐点进行卷积，将上一步的卷积结果混合到输出通道中。

Deconvolution2D层：该层是卷积操作的转置（反卷积）。需要反卷积的情况通常发生在用户想要对一个普通卷积的结果做反方向的变换。例如，将具有该卷积层输出shape的tensor转换为具有该卷积层输入shape的tensor。

Cropping1D层：在时间轴（axis1）上对1D输入（即时间序列）进行裁剪

Cropping2D层：对2D输入（图像）进行裁剪，将在空域维度，即宽和高的方向上裁剪

Cropping3D层：对2D输入（图像）进行裁剪

UpSampling1/2/3D层：不明所以

ZeroPadding1D层：对1D输入的首尾端（如时域序列）填充0，以控制卷积以后向量的长度

ZeroPadding2D层：对2D输入（如图片）的边界填充0，以控制卷积以后特征图的大小

ZeroPadding3D层：将数据的三个维度上填充0

Pooling

MaxPooling1D层：对时域1D信号进行最大值池化

MaxPooling2D层：为空域信号施加最大值池化

MaxPooling3D层：为3D信号（空域或时空域）施加最大值池化

AveragePooling1/2/3D层

GlobalMaxPooling1/2D层

GlobalAveragePooling1/2D层

LocallyConnceted

LocallyConnected1/2D层：和 Convolution1/2D工作方式类似，唯一不同的是不进行权值共享。
Recurrent

Recurrent层：这是递归层的抽象类，不能实例化，请使用它的子类：LSTM/SimpleRNN

SimpleRNN层：全连接RNN网络，RNN的输出会被回馈到输入

GRU层：门限递归单元（详见参考文献）

LSTM层：Keras长短期记忆模型，关于此算法的详情，请参考 本教程

Embedding

Embedding层：嵌入层将正整数（下标）转换为具有固定大小的向量，如[[4],[20]]->[[0.25,0.1],[0.6,-0.2]]，Embedding层只能作为模型的第一层
Advanced Activation

LeakyReLU层：LeakyRelU是修正线性单元（Rectified Linear Unit，ReLU）的特殊版本，当不激活时，LeakyReLU仍然会有非零输出值，从而获得一个小梯度，避免ReLU可能出现的神经元“死亡”现象。

PReLU层：该层为参数化的ReLU（Parametric ReLU）

ELU层：ELU层是指数线性单元（Exponential Linera Unit）

ParametricSoftplus层：该层是参数化的Softplus

ThresholdedReLU层：该层是带有门限的ReLU

SReLU层：该层是S形的ReLU

BatchNormalization

BatchNormalization层：该层在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1，具体请参考BN算法。
Noise

GaussianNoise层：为层的输入施加0均值，标准差为sigma的加性高斯噪声。

GaussianDropout层：为层的输入施加以1为均值，标准差为sqrt(p/(1-p)的乘性高斯噪声

Wrapper

TimeDistributed包装器：该包装器可以把一个层应用到输入的每一个时间步上

Bidirectional包装器：双向RNN包装器