(1)10折交叉验证（来自第六章）
代码：
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold

#创建模型 for scikit-learn
model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)

# 10折交叉验证
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)
results = cross_val_score(model, x, Y, cv=kfold)




(2)GridSearchCV 模型调参： 需要一个字典类型的字段作为需要调参的参数------------调参
代码：

from sklearn.model_selection import GridSearchCV
from keras.wrappers.scikit_learn import KerasClassifier

# 构建需要调参的参数
param_grid = {}
param_grid['optimizer'] = ['rmsprop', 'adam']
param_grid['init'] = ['glorot_uniform', 'normal', 'uniform']
param_grid['epochs'] = [50, 100, 150, 200]
param_grid['batch_size'] = [5, 10, 20]

# 调参
grid = GridSearchCV(estimator=model, param_grid=param_grid)
results = grid.fit(x, Y)

# 输出结果
print('Best: %f using %s' % (results.best_score_, results.best_params_))
means = results.cv_results_['mean_test_score']
stds = results.cv_results_['std_test_score']
params = results.cv_results_['params']

for mean, std, param in zip(means, stds, params):
    print('%f (%f) with: %r' % (mean, std, param))



(3)Pipline 便于交叉验证的每一个折中执行数据标准化处理 --------------数据标准化
# 数据正态化，改进算法
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

steps = []
steps.append(('standardize', StandardScaler()))
steps.append(('mlp', model))
pipeline = Pipeline(steps)
kfold = KFold(n_splits=10, shuffle=True, random_state=seed)
results = cross_val_score(pipeline, x, Y, cv=kfold)
print('Standardize: %.2f (%.2f) MSE' % (results.mean(), results.std()))





(4) StandardScaler()
构建神经网络模型，对数据进行标准化处理是一种很好的数据准备方法。
数据标准化是对数据进行缩放，使得每个属性的平均值为0，标准差为1，且使得数据保持高斯分布。同时对每个属性的中心倾向进行规范化，因为数据标准化后符合高斯分布，又叫数据正态化。
new_x = StandardScaler().fit_transform(x)




（5）train_test_split()
train_test_split是交叉验证中常用的函数，功能是从样本中随机的按比例选取train_data和test_data，形式为：
X_train,X_test, y_train, y_test =cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0)

cross_validatio为交叉验证

参数解释：
train_data：所要划分的样本特征集
train_target：所要划分的样本结果
test_size：样本占比，如果是整数的话就是样本的数量
random_state：是随机数的种子。
随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。
随机数的产生取决于种子，随机数和种子之间的关系遵从以下两个规则：
种子不同，产生不同的随机数；种子相同，即使实例不同也产生相同的随机数。

代码：
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)





(6)scaler.fit_transform    MinMaxScaler   标准化数据
from sklearn.preprocessing import MinMaxScaler

    scaler = MinMaxScaler()
    dataset = scaler.fit_transform(dataset)

(7) scaler.inverse_transform     反标准化数据-> 目的是保证MSE的准确性
predict_train = scaler.inverse_transform(predict_train)