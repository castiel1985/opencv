
GRU是什么
GRU即Gated Recurrent Unit。前面说到为了克服RNN无法很好处理远距离依赖而提出了LSTM，而GRU则是LSTM的一个变体，
当然LSTM还有有很多其他的变体。GRU保持了LSTM的效果同时又使结构更加简单，所以它也非常流行。


GRU模型

回顾一下LSTM的模型，LSTM的重复网络模块的结构很复杂，它实现了三个门计算，即遗忘门、输入门和输出门。


更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。
重置门用于控制忽略前一时刻的状态信息的程度，重置门的值越小说明忽略得越多。




标准LSTM和GRU的差别并不大，但是都比tanh要明显好很多，所以在选择标准LSTM或者GRU的时候还要看具体的任务是什么。
使用LSTM的原因之一是解决RNN Deep Network的Gradient错误累积太多，以至于Gradient归零或者成为无穷大，所以无法继续进行优化的问题。
GRU的构造更简单：比LSTM少一个gate，这样就少几个矩阵乘法。在训练数据很大的情况下GRU能节省很多时间。



区别
1. 对memory 的控制
LSTM： 用output gate 控制，传输给下一个unit
GRU：直接传递给下一个unit，不做任何控制
2. input gate 和reset gate 作用位置不同
LSTM: 计算new memory c^(t)
时 不对上一时刻的信息做任何控制，而是用forget gate 独立的实现这一点
GRU: 计算new memory h^(t)
时利用reset gate 对上一时刻的信息 进行控制。
3. 相似
最大的相似之处就是， 在从t 到 t-1 的更新时都引入了加法。
这个加法的好处在于能防止梯度弥散，因此LSTM和GRU都比一般的RNN效果更好。