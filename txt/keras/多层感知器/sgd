sgd
随机梯度下降
支持动量参数，支持学习衰减率，支持Nesterov动量
参数

lr：大或等于0的浮点数，学习率
momentum：大或等于0的浮点数，动量参数
decay：大或等于0的浮点数，每次更新后的学习率衰减值，也即学习衰减率。
nesterov：布尔值，确定是否使用Nesterov动量

学习率衰减思想：
学习率随着训练的进行逐渐衰减。在训练开始时候，使用较大的衰减率，可以使得结果快速收敛，随着训练的进行，逐步降低学习率和收敛的速度，有助于获得最优结果。
特点：早期快速学习，逐步进行微调，直到找到最优值。

初始学习率通常设置为0.1。 

（1）学习率线性衰减，
通过SGD类的随机梯度下降优化算法实现。只有一个decay 衰减率参数。

（2）学习率指数衰减。
通过在固定的epoch周期将学习率降低50%来实现。
比如开始学习率设置为 0.1，每10个epochs 降低50%，前十个使用0.1的学习率，接下来的10个epochs使用0.05的学习率。学习率以指数级进行衰减。


学习率衰减使用技巧：
（1）提高初始学习率。开始时快速更新权值，随着学习，可提高梯度下降性能。
（2）使用大动量。有助于在优化算法在学习率缩小到小值时，继续向正确的方向更新权值。