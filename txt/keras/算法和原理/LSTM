
LSTM的展开结构中与RNN的不同主要是存在控制存储状态的结构，其结构构成与作用如下所示。我们先将这个重复结构叫做一个单元，
把该单元的一次计算作为一拍，那么这个单元存在三组重要的变量：单元输入、单元输出以及单元状态（Cell State），
从直观来说，LSTM的作用是：
1. 根据上一拍单元的输出ht-1以及本拍的输入xt，计算出需要单元状态中需要遗忘的元素，这种控制是通过门（Gate, ）实现的；
2. 根据上一拍输出ht-1与本拍输入xt选择需要在单元状态中新记忆的状态；
3. 根据单元状态和本拍输入输出ht。
通过这样的操作，该单元就将需要记忆的信息通过单元状态的形式记录下来并且在神经元中进行传递，而每一次计算，
神经元都可以根据当前的输入对单元状态进行修改。

组件：
Cell，就是我们的小本子，有个叫做state的参数东西来记事儿的
Input Gate，Output Gate，在参数输入输出的时候起点作用，算一算东西
Forget Gate：不是要记东西吗，咋还要Forget呢。这个没找到为啥就要加入这样一个东西，因为原始的LSTM在这个位置就是一个值1，是连接到下一时间的那个参数，估计是以前的事情记太牢了，最近的就不住就不好了，所以要选择性遗忘一些东西。

（1）遗忘门
在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为忘记门层完成。该门会读取ht−1和xt，输出一个在 0到 1之间的数值给每个在细胞状态 Ct−1 中的数字。
1 表示“完全保留”，0 表示“完全舍弃”。
让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。

（2）输入门
下一步是决定让多少新的信息加入到 cell 状态 中来。实现这个需要包括两个 步骤：首先，一个叫做“input gate layer ”的 sigmoid 层决定哪些信息需要更新；一个 tanh 层生成一个向量，也就是备选的用来更新的内容，C^t 。
在下一步，我们把这两部分联合起来，对 cell 的状态进行一个更新。
在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。



（3）输出门
最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态
通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。
在语言模型的例子中，因为他就看到了一个 代词，可能需要输出与一个 动词 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。




两个关键问题：

1.      为什么具有记忆功能？
这个是在RNN就解决的问题，就是因为有递归效应，上一时刻隐层的状态参与到了这个时刻的计算过程中，
直白一点呢的表述也就是选择和决策参考了上一次的状态。

2.      为什么LSTM记的时间长？
因为特意设计的结构中具有CEC的特点，误差向上一个上一个状态传递时几乎没有衰减，所以权值调整的时候，
对于很长时间之前的状态带来的影响和结尾状态带来的影响可以同时发挥作用，最后训练出来的模型就具有较长时间范围内的记忆功能。



一些可能存在的问题
1.     Theano LSTM 例程里的梯度回传算法，由于Theano自动求导的功能，在回传时候应该是采用的是FULL BPTT，这个是不是会带来一些如之前所说的问题？
答：这个问题我在之前其实已经说明了Alex的一些Trick的做法。仔细思考下，确实FULL BPTT带来的梯度的问题，由于Memory Cell功能的完整性，而被缓解了。也就是说误差回传的主力还是通过了Memory Cell而保持了下来。所以我们现在用的LSTM模型，依然有比较好的效果。

2.     LSTM的Input、Output、Forget门之间的功能是不是其实有重复？有没有更简单的结构可以改进。
答：没错，例如已经出现的GRU (Gated Recurrent Unit)



多层LSTM
多层LSTM是将LSTM进行叠加，其优点是能够在高层更抽象的表达特征，并且减少神经元的个数，增加识别准确率并且降低训练时间。

