最新自然语言处理(NLP)四步流程：Embed->Encode->Attend->Predict


(1)词向量
词向量表将高维的稀疏二值向量映射成低维的稠密向量。举个例子，假设我们收到的文本是一串ASCII字符，共有256种可能值，于是我们把每一种可能值表示为一个256维的二值向量。字符’a’的向量只有在第97维的值等于1，其它维度的值都等于0。
字符’b’的向量只有在第98维的值等于1，其它维度的值都等于0。这种表示方法称为’one hot’形式。不同字符的向量表示完全不一样。
大部分神经网络模型首先都会把输入文本切分成若干个词语，然后将词语都用词向量表示。另一些模型用其它信息扩展了词向量表示。比如，除了词语的ID之外，还会输入一串标签。然后可以学习得到标签向量，将标签向量拼接为词向量。
这可以让你将一些位置敏感的信息加入到词向量表示中。然而，有一个更强大的方式来使词语表示呈现出语境相关。

(2)编码
假设得到了词向量的序列，编码这一步是将其转化为句子矩阵，矩阵的每一行表示每个词在上下文中所表达的意思。

(3)注意力机制
这一步是将上一步的矩阵表示压缩为一个向量表示，因此可以被送入标准的前馈神经网络进行预测。
注意力机制对于其它压缩方法的优势在于它输入一个辅助的上下文向量：
Yang等人在2016年发表的论文提出了一种注意力机制，输入一个矩阵，输出一个向量。区别于从输入内容中提取一个上下文向量，
该机制的上下文向量是被当做模型的参数学习得到。这使得注意机制变成一个纯粹的压缩操作，可以替换任何的池化步骤。

(4)预测
文本内容被压缩成一个向量之后，我们可以学习最终的目标表达 —— 一种类别标签、一个实数值或是一个向量等等。
我们也可以将网络模型看做是状态机的控制器，如一个基于转移的解析器，来做结构化预测。



