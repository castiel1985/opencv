函数：
（1）tf.placeholder:  产生一个float占位符，默认float32
（2）tf.multiply:   将两个参数相乘，可以是数字也可以是矩阵
（3）sess.run():
    (1)只是计算几个数据，数据必须被赋值 ->  update = tf.assign(state, new_val)  #返回tensor
    (2)使用feed_dict:使用placeholder创建出来的tensor赋值  ->  sess.run(y, feed_dict={a: 1, b: 2})  #a,b都是placeholder创造出来的值

（4）tf.Variable:定义一个图变量   x = tf.Variable(3, name='x') #在TensorFlow的世界里，变量的定义和初始化是分开的，所有关于图变量的赋值和计算都要通过
tf.Session的run来进行。参数：
    initial_value 	所有可以转换为Tensor的类型 	变量的初始值
    trainable 	bool 	如果为True，会把它加入到GraphKeys.TRAINABLE_VARIABLES，才能对它使用Optimizer
    collections 	list 	指定该图变量的类型、默认为[GraphKeys.GLOBAL_VARIABLES]
    validate_shape 	bool 	如果为False，则不进行类型和维度检查
    name 	string 	变量的名称，如果没有指定则系统会自动分配一个唯一的值



（5）tf.square  对参数列表内的每一个元素求平方根，即点乘
    tf.matmul   矩阵乘法

(6)tf.global_variables_initializer() :添加节点用于初始化所有的变量(GraphKeys.VARIABLES)。返回一个初始化所有全局变量的操作（Op）,run了 所有global Variable 的 assign op，这就是初始化参数的本来面目。

(7) tf.train.GradientDescentOptimizer(0.01)  对所有步骤中的所有变量使用恒定的学习率, 即 简单随机梯度下降算法
       例子：cost = tf.square(Y - y_model) # use square error for cost function
            train_op = tf.train.GradientDescentOptimizer(0.01).minimize(cost)   # 梯度下降优化
    __init__(
        learning_rate,
        use_locking=False,
        name='GradientDescent'
    )

（8）tf.train.AdagradOptimizer    适应学习率算法，它独立地适应所有模型参数的学习率。算法维护一个累积平方梯度向量，即对模型的每个参数，累积其历史平方值的总和，以其反比作为权重更新参数
        __init__(
            learning_rate, 
            initial_accumulator_value=0.1, 
            use_locking=False, 
            name='Adagrad'
        )

(9) tf.train.AdamOptimizer      实现 Adam优化算法（  Adam 这个名字来源于 adaptive moment estimation，自适应矩估计。）
        __init__(
            learning_rate=0.001,
            beta1=0.9,
            beta2=0.999,
            epsilon=1e-08,
            use_locking=False,
            name='Adam'
        )

(10) tf.train.MomentumOptimizer  实现 动量梯度下降算法 ，动量算法引入了变量v充当速度角色。使用动量的随机梯度下降算法更新规则在原随机梯度下降基础上改进
    __init__(
        learning_rate,
        momentum,
        use_locking=False,
        name='Momentum',
        use_nesterov=False
    )
    
（11）tf.random_normal()函数用于从服从指定正太分布的数值中取出指定个数的值。  
    例子： w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))
                #shape:一维的张量，也就是输出的张量
                #mean:正态分布的均值
                stddev:正态分布的标准差
                seed:一个整数，设置之后每次生成的随机数一样
                #dtype:表示的是输出类型
    
    
(12) tf.reduce_mean  在某一个维度上求取平均值
    例子： # 'x' is [[1., 2.]
          #         [3., 4.]]
          tf.reduce_mean(x) ==> 2.5 #如果不指定第二个参数，那么就在所有的元素中取平均值
          tf.reduce_mean(x, 0) ==> [2.,  3.] #指定第二个参数为0，则第一维的元素取平均值，即每一列求平均值
          tf.reduce_mean(x, 1) ==> [1.5,  3.5] #指定第二个参数为1，则第二维的元素取平均值，即每一行求平均值
          
          
(13) tf.nn.softmax_cross_entropy_with_logits  
            1)求loss，则要做一步tf.reduce_mean;
            2)求交叉熵, 再做一步tf.reduce_sum操作,就是对向量里面所有元素求和 
       注明:第一个参数logits：就是神经网络最后一层的输出，如果有batch的话，它的大小就是[batchsize，num_classes]，单样本的话，大小就是num_classes
            3）返回值并不是一个数，而是一个向量
第二个参数labels：实际的标签，大小同上。
tensorflow之所以把softmax和cross entropy放到一个函数里计算，就是为了提高运算速度，虽然这样做让很多刚接触tf的同志难以捉摸，但Google的工程师是不会以“使用者的舒服程度”为第一要素设计程序的。
          
          
(14)激活函数
        tf.nn.relu()
        tf.nn.sigmoid()
        tf.nn.tanh()
        tf.nn.elu()
        tf.nn.bias_add()
        tf.nn.crelu()
        tf.nn.relu6()
        tf.nn.softplus()
        tf.nn.softsign()
        tf.nn.dropout()
        tf.nn.relu_layer(x, weights, biases,name=None)
        
    例子：
def model(X, w_h, w_h2, w_o, p_keep_input, p_keep_hidden): 
    X = tf.nn.dropout(X, p_keep_input)
    h = tf.nn.relu(tf.matmul(X, w_h))
    h = tf.nn.dropout(h, p_keep_hidden)
    h2 = tf.nn.relu(tf.matmul(h, w_h2))
    h2 = tf.nn.dropout(h2, p_keep_hidden)
    return tf.matmul(h2, w_o)
        
        
（15）tf.argmax()   返回最大数值的下标 
例子：
>>> a = tf.constant([1.,2.,3.,0.,9.,])
>>> b = tf.constant([[1,2,3],[3,2,1],[4,5,6],[6,5,4]])
>>> with tf.Session() as sess:
...     sess.run(tf.argmax(a, 0))



（16）tf.equal(a, b)   对比这两个矩阵或者向量的相等的元素，如果是相等的那就返回True，反正返回False，返回的值的矩阵维度和A是一样的
    例子：
    A = [[1,3,4,5,6]]
    B = [[1,3,4,3,2]]

    with tf.Session() as sess:
        print(sess.run(tf.equal(A, B))



（17）tf.train.RMSPropOptimizer  ：使用RMSProp算法的Optimizer



（18）tf.random_uniform（shape=[n_visible, n_hidden],
                           minval=-W_init_max,
                           maxval=W_init_max）  ：
    例子：tf.random_uniform((4, 4), minval=low,maxval=high,dtype=tf.float32)))返回4*4的矩阵，产生于low和high之间，产生的值是均匀分布的。

    
    np.random.shuffle  现场修改序列，改变自身内容。（类似洗牌，打乱顺序）


（19）tf.zeros（shape, dtype=tf.float32, name=None）
例子：
        1. 一维数组里放一个值
        import tensorflow as tf
        res = tf.random_uniform((4, 4), -1, 1)
        res2 = tf.zeros([1])
        with tf.Session() as sess:
            print(sess.run(res2))
        #结果为：
        [0.]

        2. 一维数组里放两个值
        import tensorflow as tf
        res = tf.random_uniform((4, 4), -1, 1)
        res2 = tf.zeros([2])
        with tf.Session() as sess:
            print(sess.run(res2))
        #结果为：
        [0. 0.]

        3. 二维数组
        import tensorflow as tf
        res = tf.random_uniform((4, 4), -1, 1)
        res2 = tf.zeros([2, 4])
        with tf.Session() as sess:
            print(sess.run(res2))
        #结果为：
        [[0. 0. 0. 0.]
         [0. 0. 0. 0.]]
         
 （20） np.random.binomial(n,p,size=None)  返回的是：一个数字或者一组数字,每个样本返回的是n次试验中事件A发生的次数。  
     例子：  >>> n, p = 2, .5            # 两枚都是正面
            >>> sum(np.random.binomial(n, p, size=20000)==2)/20000.
                0.25019999999999998         # 和我们的精确概率值相接近
            # 其中一个为反面
            >>> sum(np.random.binomial(n, p, size=20000)==1)/20000.
                0.49430000000000002
 
 

    (21)ZIP函数  zip 方法在 Python 2 和 Python3中的不同：在 Python 3.x 中为了减少内存，zip() 返回的是一个对象。如需展示列表，需手动 list() 转换。
        >>>a = [1,2,3]
        >>> b = [4,5,6]
        >>> c = [4,5,6,7,8]
        >>> zipped = zip(a,b)     # 打包为元组的列表
        [(1, 4), (2, 5), (3, 6)]
        >>> zip(a,c)              # 元素个数与最短的列表一致
        [(1, 4), (2, 5), (3, 6)]
        >>> zip(*zipped)          # 与 zip 相反，*zipped 可理解为解压，返回二维矩阵式
        [(1, 2, 3), (4, 5, 6)]
        
   （22）range(start, stop[, step])   
        >>> range(0, 10, 3)  # 步长为 3
            [0, 3, 6, 9]
            
    (23)tf.transpose（x, perm=[0, 2, 1]）   转置，对于低维度的转置问题，很简单；perm=[0,1,2],0代表三维数组的高（即为二维数组的个数），1代表二维数组的行，2代表二维数组的列
    
    
    (24)tf.reshape(tensor, shape, name=None) 
    # tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]
    # tensor 't' has shape [9]
    reshape(t, [3, 3]) ==> [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]
    # tensor 't' is [[[1, 1], [2, 2]],
    #                [[3, 3], [4, 4]]]
    # tensor 't' has shape [2, 2, 2]
    reshape(t, [2, 4]) ==> [[1, 1, 2, 2],
                            [3, 3, 4, 4]]


    (25)tf.nn.embedding_lookup 函数    选取一个张量里面索引对应的元素。tf.nn.embedding_lookup（tensor, id）:tensor就是输入张量，id就是张量对应的索引



    (26)enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。
    
    (27)tf.variable_scope可以让变量有相同的命名，包括tf.get_variable得到的变量，还有tf.Variable的变量

        tf.name_scope可以让变量有相同的命名，只是限于tf.Variable的变量
        
     (28)tf.Variable()   
             W = tf.Variable(<initial-value>, name=<optional-name>)  用于生成一个初始值为initial-value的变量。必须指定初始化值
         tf.get_variable()
             W = tf.get_variable(name, shape=None, dtype=tf.float32, initializer=None,regularizer=None, trainable=True, collections=None) 获取已存在的变量（要求不仅名字，而且初始化方法等各个参数都一样），如果不存在，就新建一个。 
可以用各种初始化方法，不用明确指定值。

        方便共享变量:因为tf.get_variable() 会检查当前命名空间下是否存在同样name的变量，可以方便共享变量。而tf.Variable 每次都会新建一个变量。

需要注意的是tf.get_variable() 要配合reuse和tf.variable_scope() 使用.


    (29)f.Variable()和 tf.get_variable() 初始化  
a1 = tf.Variable(1, name='a1')                                                # 用变量初始化
a2 = tf.Variable(tf.constant(1), name='a2')                                   # 用变量初始化
a3 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a3')  # 用随机数初始化 
a4 = tf.Variable(tf.ones(shape=[2,3]), name='a4')                             # 用one矩阵初始化

# 定义常量
a5 = tf.get_variable(name='a5', initializer=22) 
# 定义正态分布的随机变量 
a6 = tf.get_variable(name='a6', shape=[2,3], initializer=tf.random_normal_initializer(mean=0, stddev=1))  
# 定义常量向量
a7 = tf.get_variable(name='a7', shape=[1], initializer=tf.constant_initializer(1))  
# 全是1的矩阵

init = tf.global_variables_initializer()  #输出数据时必须要全局初始化 
with tf.Session() as sess:  
    sess.run(init)  
    print sess.run(a5)  
    print sess.run(a6)  
    print sess.run(a7)  
    print sess.run(a8

tf.get_variable(name,  shape, initializer): name就是变量的名称，shape是变量的维度，initializer是变量初始化的方式，初始化的方式有以下几种：
tf.constant_initializer：常量初始化函数
tf.random_normal_initializer：正态分布
tf.truncated_normal_initializer：截取的正态分布
tf.random_uniform_initializer：均匀分布
tf.zeros_initializer：全部是0
tf.ones_initializer：全是1
tf.uniform_unit_scaling_initializer：满足均匀分布，但不影响输出数量级的随机值


(30)tf.summary :https://www.cnblogs.com/lyc-seu/p/8647792.html


(31)tf.train.Saver()  保存和恢复都需要实例化一个 tf.train.Saver
    saver.restore(sess, 'tmp/model.ckpt')    #存在就从模型中恢复变量
    save_path = saver.save(sess, 'tmp/model.ckpt')  #保存模型



(32)np.linspace  在指定的间隔内返回均匀间隔的数字。
    >>> np.linspace(1, 10, 10)
    array([  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.])
    >>> np.linspace(1, 10, 10, endpoint = False)
    array([ 1. ,  1.9,  2.8,  3.7,  4.6,  5.5,  6.4,  7.3,  8.2,  9.1])






    
    
    
    
    
    
    
    