
回归算法用于连续型分布预測。针对的是数值型的样本，使用回归。
能够在给定输入的时候预測出一个数值。这是对分类方法的提升，由于这样能够预測连续型数据而不不过离散的类别标签。

回归的目的就是建立一个回归方程用来预測目标值。回归的求解就是求这个回归方程的回归系数。预測的方法当然十分简单，
回归系数乘以输入值再所有相加就得到了预測值。

回归最简单的定义是，给出一个点集D，用一个函数去拟合这个点集。而且使得点集与拟合函数间的误差最小，假设这个函数曲线是一条直线，
那就被称为线性回归，假设曲线是一条二次曲线，就被称为二次回归。


简单线性回归模型:
（1）第一步 确定变量
根据预测目标，确定自变量和因变量

问题：投入60万的推广费，能够带来多少的销售额？

确定因变量和自变量很简单，谁是已知，谁就是自变量，谁是未知，就就是因变量，因此，推广费是自变量，销售额是因变量；

(2）第二步 确定类型
绘制散点图，确定回归模型类型

根据前面的数据，画出自变量与因变量的散点图，看看是否可以建立回归方程，在简单线性回归分析中，我们只需要确定自变量与因变量的相关度为强相关性，即可确定可以建立简单线性回归方程，根据jacky前面的文章分享《Python相关分析》，我们很容易就求解出推广费与销售额之间的相关系数是0.94，
也就是具有强相关性，从散点图中也可以看出，二者是有明显的线性相关的，也就是推广费越大，销售额也就越大

（3）第三步 建立模型
估计模型参数，建立回归模型
要建立回归模型，就要先估计出回归模型的参数A和B，那么如何得到最佳的A和B，使得尽可能多的数据点落在或者更加靠近这条拟合出来的直线上呢？
统计学家研究出一个方法，就是最小二乘法，最小二乘法又称最小平方法，通过最小化误差的平方和寻找数据的最佳直线，这个误差就是实际观测点和估计点间的距离；
最小二乘法名字的缘由有二个：一是要将误差最小化，二是使误差最小化的方法是使误差的平方和最小化；在古汉语中，平方称为二乘，用平方的原因就是要规避负数对计算的影响，所以最小二乘法在回归模型上的应用就是要使得实际观测点和估计点的平方和达到最小，也就是上面所说的使得尽可能多的数据点落在或者说更加靠近这条拟合出来的直线上；
/*
#估计模型参数，建立回归模型
'''
(1) 首先导入简单线性回归的求解类LinearRegression
(2) 然后使用该类进行建模，得到lrModel的模型变量
'''

lrModel = LinearRegression()
#(3) 接着，我们把自变量和因变量选择出来
x = data[['活动推广费']]
y = data[['销售额']]

#模型训练
'''
调用模型的fit方法，对模型进行训练
这个训练过程就是参数求解的过程
并对模型进行拟合
'''
lrModel.fit(x,y)

*/

（4）第四步 模型检验
对回归模型进行检验

回归方程的精度就是用来表示实际观测点和回归方程的拟合程度的指标，使用判定系数来度量。
#对回归模型进行检验
lrModel.score(x,y)
执行代码可以看到，模型的评分为0.887,是非常不错的一个评分，我们就可以使用这个模型进行未知数据的预测了

（5）第五步 模型预测
调用模型的predict方法，这个就是使用sklearn进行简单线性回归的求解过程；
lrModel.predict([[60],[70]])
补充：
/*
如果需要获取到拟合出来的参数各是多少，可以使用模型的intercept属性查看参数a(截距)，使用coef属性查看参数b
#查看截距
alpha = lrModel.intercept_[0]
#查看参数
beta = lrModel.coef_[0][0]

alpha + beta*numpy.array([60,70])
*/

完整代码:
/*
import numpy
from pandas import read_csv
from matplotlib import pyplot as plt
from sklearn.linear_model import LinearRegression

data = read_csv(
    'file:///Users/apple/Desktop/jacky_1.csv',encoding='GBK'
)

#画出散点图，求x和y的相关系数
plt.scatter(data.活动推广费,data.销售额)

data.corr()

#估计模型参数，建立回归模型
'''
(1) 首先导入简单线性回归的求解类LinearRegression
(2) 然后使用该类进行建模，得到lrModel的模型变量
'''

lrModel = LinearRegression()
#(3) 接着，我们把自变量和因变量选择出来
x = data[['活动推广费']]
y = data[['销售额']]

#模型训练
'''
调用模型的fit方法，对模型进行训练
这个训练过程就是参数求解的过程
并对模型进行拟合
'''
lrModel.fit(x,y)

#对回归模型进行检验
lrModel.score(x,y)

#利用回归模型进行预测
lrModel.predict([[60],[70]])

#查看截距
alpha = lrModel.intercept_[0]

#查看参数
beta = lrModel.coef_[0][0]

alpha + beta*numpy.array([60,70])
*/

总结-sklearn建模流程:
1)建立模型
lrModel = sklearn.linear_model.LinearRegression()
2)训练模型
lrModel.fit(x,y)
3)模型评估
lrModel.score(x,y)
4)模型预测
lrModel.predict(x)





多重线性回归模型:
(1)研究一个因变量与多个自变量间线性关系的方法
(2)在实际工作中，因变量的变化往往受几个重要因素的影响，此时就需要用2个或2个以上的影响因素作为自变量来解释因变量的变化，这就是多重线性回归;

案例实操-金融场景
下面，jacky通过一个金融场景的案例，开始我们的分享：某金融公司打算新开一类金融产品，现有9个金融产品的数据，包括用户购买金融产品的综合年化利率，以及公司收取用户的佣金（手续费）；
如下表所示，产品利率为11％，佣金为50，我们需要预测这款金融产品的销售额

产品编号	百分比利率	抽取用户佣金	金融产品销售额
1	    9       	75          500
2	    7	        30      	370
3	    7       	20      	375
4	    5       	30      	270
5	    6       	0       	360
6	    7       	21      	379
7	    8       	50      	440
8	    6       	20      	300
9	    9       	60      	510
10	    11      	50      	？


第一步 确定变量
根据预测目标，确定自变量和因变量
因变量：销售额
自变量：利率、佣金

第二步 确定类型
绘制散点图，确定回归模型类型
从散点图和相关系数结果表可以看出，产品利率和销售额是强正相关；佣金与销售额是强负相关；因此，我们可以使用多重线性模型来解决这个问题；


第三步 建立模型
估计模型参数，建立回归模型

多重线性回归模型参数的估计方法与简单线性回归模型参数的估计方法是相同的：都是采用最小二乘法进行估计（对最小二乘法更详细的解析，
请参见Python回归分析五步曲（一）—简单线性回归）

第四步 模型检验
对回归模型进行检验
解析：拟合完方程的参数之后，我们就要对回归模型进行检验，在简单线性回归的分享中，我们用判定系数来验证方程的拟合程度，而在多重线性回归中，如果在模型中增加一个自变量，模型中R平方往往也会相应增加，这就会给我们一个错觉：要使得模型拟合的好，只要增加自变量即可。因此，使用判定系数R平方来验证方程的拟合程度是不够科学的，需要自变量个数进行修正和调整，也就是调整判定系数；以上，我们只要理解原理即可，
公式记不住也不要紧，知道多重线性模型需要用调整判定系数来判定方程的拟合程度，会用Python看结果就可以了。

第五步 模型预测
利用回归模型进行预测
根据已有的自变量数据，预测需要的因变量对应的结果


完整代码
/*
import pandas
data = pandas.read_csv(
    'file:///Users/apple/Desktop/jacky_1.csv',encoding='GBK'
)

import matplotlib
from pandas.tools.plotting import scatter_matrix
font = {
    'family':'SimHei'
}
matplotlib.rc('font',**font)

scatter_matrix(
    data[["百分比利率","抽取用户佣金","金融产品销售额"],
    figsize =(10,10),diagonal = 'kid'
)

data[["百分比利率","抽取用户佣金","金融产品销售额"]].corr()
x = data[["百分比利率","抽取用户佣金"]]
y = data[["金融产品销售额"]]

#建模
from sklearn.linear_model import LinearRegression
lrModel = LinearRegression()

#训练模型
lrModel.fit(x,y)

#预测
lrModel.predict([11,50])

#查看参数
lrModel.coef_

#查看截距
lrModel.intercept_
*/